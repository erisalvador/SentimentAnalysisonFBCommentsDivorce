{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs and Imports"
      ],
      "metadata": {
        "id": "e7W-0I7MNDJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic fuzzywuzzy emoji langdetect transformers"
      ],
      "metadata": {
        "id": "C2QEk0bOSnSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import glob\n",
        "import itertools\n",
        "import json\n",
        "import re\n",
        "import emoji\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "from langdetect import detect\n",
        "from langdetect import detect_langs\n",
        "from langdetect import DetectorFactory\n",
        "DetectorFactory.seed = 0\n",
        "import time\n",
        "import logging\n",
        "\n",
        "## Third-party imports\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.graph_objects as go\n",
        "import scipy.sparse\n",
        "from bertopic import BERTopic\n",
        "from fuzzywuzzy import process\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import (accuracy_score, classification_report, cohen_kappa_score, f1_score, precision_score, recall_score)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from umap import UMAP\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, Conv1D, GlobalMaxPooling1D\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
        "\n",
        "# NLTK-specific imports\n",
        "from nltk.util import ngrams"
      ],
      "metadata": {
        "id": "v2_SAAo8SW-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'xlm-roberta-base'  # multilingual model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Zb1CDHztK_Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drive Mounting"
      ],
      "metadata": {
        "id": "_xA81w3JaFq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IfT3HgXSaHsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/Research/SentimentAnalysisDivorce/Dataset/FINAL_DATASET.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "q1v-B0_zaJNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "id": "mNO2IewoTHfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "CJ3_JC92TeiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise Reduction"
      ],
      "metadata": {
        "id": "NH5kTfi19KSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removal of Duplicates"
      ],
      "metadata": {
        "id": "aqT9Ct64TLIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame with only the specified columns\n",
        "selected_columns = [\"profileId\", \"profileName\", \"replyToCommentId\", \"text\"]\n",
        "df_selected = df[selected_columns]"
      ],
      "metadata": {
        "id": "y9yuyuTZQyI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "df_selected.drop_duplicates(subset='text', inplace=True)\n",
        "df_selected.to_csv('PREPROCESSED_DATA_01.csv', index=False)"
      ],
      "metadata": {
        "id": "c1W8XN3mRQ_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_selected.shape)"
      ],
      "metadata": {
        "id": "yHQSDCGKTs4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to Google Drive\n",
        "output_file_path = '/content/drive/My Drive/Research/SentimentAnalysisDivorce/Dataset/PREPROCESSED_DATA_01.csv'\n",
        "df_selected.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"File saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "ffngHB3NUZUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty DataFrame\n",
        "df = pd.DataFrame(columns=[\"profileId\", \"profileName\", \"replyToCommentId\", \"text\"])\n",
        "\n",
        "# Function to log bad lines\n",
        "bad_lines = []\n",
        "\n",
        "# Iterate over CSV files\n",
        "for filepath in glob.iglob(file_path):\n",
        "    try:\n",
        "        # Read CSV file. Should be able to handle quotes and line breaks\n",
        "        temp_df = pd.read_csv(filepath, quotechar='\"', escapechar='\\\\', on_bad_lines='skip')\n",
        "        # Extract post_id from the filename\n",
        "        post_id = filepath.split(\"\\\\\")[-1].split(\".\")[0]\n",
        "        temp_df[\"post_id\"] = post_id\n",
        "        # Append to the main DataFrame\n",
        "        df = pd.concat([df, temp_df], ignore_index=True)\n",
        "    except pd.errors.ParserError as e:\n",
        "        print(f\"Error reading {filepath}: {e}\")\n",
        "\n",
        "# Select specific columns and shuffle the DataFrame\n",
        "df = df[[\"profileId\", \"profileName\", \"replyToCommentId\", \"text\"]].sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "XH-NCkiPUWsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "df.drop_duplicates(subset=['text'], inplace=True)"
      ],
      "metadata": {
        "id": "D1vXnq9s95Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "id": "lqiw6Yb-96vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_spam(text):\n",
        "    if isinstance(text, str):\n",
        "        # Define phrases associated with spam\n",
        "        spam_phrases = [\n",
        "            'crypto', 'cryptocurrency', 'bitcoin', 'blockchain',\n",
        "            'click here', 'check my profile', 'tag for', 'follow for', 'check out',\n",
        "            'lotto', 'gambling'\n",
        "        ]\n",
        "        # Check if any spam phrase is present in the comment\n",
        "        for phrase in spam_phrases:\n",
        "            if phrase.lower() in text.lower():\n",
        "                return ''\n",
        "        # If no spam phrase is found, return the original text\n",
        "        return text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Remove spam comments\n",
        "df['text'] = df['text'].apply(remove_spam)"
      ],
      "metadata": {
        "id": "K3gw53kl-ERO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove emojis, photos, and GIFs\n",
        "def remove_non_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Define regex pattern for detecting emojis, photos, and GIFs\n",
        "        non_text_pattern = r'^[^\\x00-\\x7F]+$|http\\S+|\\bimg\\b|\\bphoto\\b|\\bgif\\b'\n",
        "        # Remove non-text elements from text\n",
        "        return re.sub(non_text_pattern, '', text).strip()\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# Remove emojis, photos, and GIFs\n",
        "df['text'] = df['text'].apply(remove_non_text)"
      ],
      "metadata": {
        "id": "kDx0ToRHAVk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "id": "8EdPJwxHAgyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect language and remove non-English/Filipino text\n",
        "def filter_language(text):\n",
        "    try:\n",
        "        langs = detect_langs(text)\n",
        "        # Check if English or Filipino is detected and its probability is greater than a threshold\n",
        "        for lang in langs:\n",
        "            if lang.lang == 'en' and lang.prob > 0.5:\n",
        "                return True\n",
        "            elif lang.lang == 'tl' and lang.prob > 0.5:\n",
        "                return True\n",
        "        return False\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Remove non-English/Filipino text\n",
        "df = df[df['text'].apply(filter_language)]"
      ],
      "metadata": {
        "id": "TEZus8g-AiYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df[df['text'].apply(filter_language)]"
      ],
      "metadata": {
        "id": "IHhw_KVhQMY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the filtered dataset to the specified location\n",
        "output_file_path = '/content/drive/My Drive/Research/SentimentAnalysisDivorce/Dataset/FILTERED_DATASET.csv'\n",
        "df_filtered.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Filtered dataset saved to {output_file_path}\")\n",
        "print(f\"Filtered dataset shape: {df_filtered.shape}\")"
      ],
      "metadata": {
        "id": "h53hMwiXQQe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Comments"
      ],
      "metadata": {
        "id": "wana3gkRHqo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging with flushing\n",
        "class FlushHandler(logging.StreamHandler):\n",
        "    def emit(self, record):\n",
        "        super().emit(record)\n",
        "        self.flush()\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[FlushHandler()])"
      ],
      "metadata": {
        "id": "YdgLsy24f-63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove emojis\n",
        "def remove_emojis(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text  # Return the original value if it's not a string\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ],
      "metadata": {
        "id": "ImwT8ZDoH_uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove special characters such as links, hashtags, question marks, exclamation points, mention tags\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text  # Return the original value if it's not a string\n",
        "    text = re.sub(r'http\\S+|www\\S+|#|\\?|!|@[\\w_]+', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "vopQVBXuIkp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to correct spelling using the pretrained ROBERTA model\n",
        "def correct_spelling(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text  # Return the original value if it's not a string\n",
        "\n",
        "    max_length = 512  # Maximum sequence length for the model\n",
        "    encoded_input = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=max_length)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(encoded_input)\n",
        "\n",
        "    predictions = outputs.logits\n",
        "    predicted_ids = predictions.argmax(dim=-1).squeeze().tolist()\n",
        "    corrected_text = tokenizer.decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "    return corrected_text"
      ],
      "metadata": {
        "id": "PC_UPi3YKOoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove mentions\n",
        "def remove_mentions(row, names_set):\n",
        "    text = row['text']\n",
        "    if pd.notna(row['replyToCommentId']):\n",
        "        if not isinstance(text, str):\n",
        "            return text  # Return the original value if it's not a string\n",
        "\n",
        "        words = text.split()\n",
        "        for i in range(1, 6):  # Check the first 1 to 5 words\n",
        "            potential_mention = ' '.join(words[:i]).lower()\n",
        "            if potential_mention in names_set:\n",
        "                return ' '.join(words[i:])\n",
        "    return text"
      ],
      "metadata": {
        "id": "DwQp4jVbTA4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process text in batches and save the results incrementally\n",
        "def process_spelling_correction_in_batches(df, batch_size, output_file, names_set):\n",
        "    num_batches = len(df) // batch_size + 1\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_num in range(num_batches):\n",
        "        batch_start_time = time.time()\n",
        "        start_idx = batch_num * batch_size\n",
        "        end_idx = min((batch_num + 1) * batch_size, len(df))\n",
        "        df_batch = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        df_batch['text'] = df_batch.apply(lambda row: remove_mentions(row, names_set), axis=1)\n",
        "        df_batch['text'] = df_batch['text'].apply(correct_spelling)\n",
        "\n",
        "        if batch_num == 0:\n",
        "            df_batch.to_csv(output_file, index=False)\n",
        "        else:\n",
        "            df_batch.to_csv(output_file, mode='a', header=False, index=False)\n",
        "\n",
        "        elapsed_time = time.time() - batch_start_time\n",
        "        total_elapsed_time = time.time() - start_time\n",
        "        remaining_batches = num_batches - (batch_num + 1)\n",
        "        estimated_remaining_time = remaining_batches * elapsed_time\n",
        "\n",
        "        logging.info(f'Processed batch {batch_num + 1}/{num_batches} in {elapsed_time:.2f} seconds.')\n",
        "        logging.info(f'Estimated remaining time: {estimated_remaining_time // 60:.0f} minutes and {estimated_remaining_time % 60:.0f} seconds.')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/My Drive/Research/SentimentAnalysisDivorce/Dataset/FINAL_DATASET.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "logging.info(f'Loaded dataset with shape: {df.shape}')\n",
        "\n",
        "# Apply preprocessing steps\n",
        "df['text'] = df['text'].apply(remove_emojis)\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "df['text'] = df['text'].str.lower()\n",
        "\n",
        "# Save the intermediate result\n",
        "intermediate_file_path = '/content/drive/My Drive/Research/SentimentAnalysisDivorce/Dataset/FINAL_DATASET_INTERMEDIATE.csv'\n",
        "df.to_csv(intermediate_file_path, index=False)\n",
        "logging.info(f'Saved intermediate dataset with shape: {df.shape}')"
      ],
      "metadata": {
        "id": "NATwZtbaPhk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a set of profile names in lowercase for faster lookup\n",
        "names_set = set(df['profileName'].str.lower().dropna().unique())"
      ],
      "metadata": {
        "id": "Z0_kDiQDMBgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the dataset in batches and save the results incrementally\n",
        "output_file = '/content/drive/My Drive/Research/SentimentAnalysisDivorce/Dataset/FINAL_DATASET_SPELLING_CORRECTED.csv'\n",
        "batch_size = 1000  # Adjust batch size as needed\n",
        "process_spelling_correction_in_batches(df, batch_size, output_file, names_set)\n",
        "\n",
        "logging.info(\"Processing complete.\")"
      ],
      "metadata": {
        "id": "HZaVVKsRPnma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape()"
      ],
      "metadata": {
        "id": "uO19X9lIk01b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}